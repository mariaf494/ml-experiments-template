{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reporte\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "from functools import partial"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import joblib\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import data\r\n",
    "import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import json\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def evaluate_model(*, model, metric, X_train, y_train, X_test, y_test):\r\n",
    "    train_predictions = model.predict(X_train)\r\n",
    "    test_predictions = model.predict(X_test)\r\n",
    "    train_error = metric(y_train, train_predictions)\r\n",
    "    test_error = metric(y_test, test_predictions)\r\n",
    "    return {\r\n",
    "        \"train_predictions\": train_predictions,\r\n",
    "        \"test_predictions\": test_predictions,\r\n",
    "        \"train_error\": train_error,\r\n",
    "        \"test_error\": test_error\r\n",
    "    }\r\n",
    "\r\n",
    "def print_report(*, model, evaluation):\r\n",
    "    print(f\"Model used:\\n\\t{reg}\")\r\n",
    "    print(f\"Error:\\n\\ttrain set {evaluation['train_error']}\\n\\ttest error: {evaluation['test_error']}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_categorical_column_names(json_path: str):\r\n",
    "    with open(json_path) as json_file:\r\n",
    "        cols_dict = json.load(json_file)\r\n",
    "    return cols_dict['cate_cols']\r\n",
    "def define_x_y_groups(df):\r\n",
    "    map_dict = {}\r\n",
    "    for i, camp in enumerate(df.AÑO_CAMPAÑA.unique()):\r\n",
    "        map_dict[camp] = i\r\n",
    "    groups = df.AÑO_CAMPAÑA.map(map_dict)\r\n",
    "    y = df.CANTPED\r\n",
    "    X = df.drop(['CANTPED', \"AÑO_CAMPAÑA\"], axis=1)\r\n",
    "    return X, y, groups"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "models_dir = \"models\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dataset_path = \"df_1.csv\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "df = pd.read_csv('df_1.csv')\r\n",
    "y = df[\"CANTPED\"]\r\n",
    "X = df.drop(columns=['CANTPED', \"AÑO_CAMPAÑA\"])\r\n",
    "X = X.astype(\r\n",
    "    {k: str for k in get_categorical_column_names('modelcols_1.json')})\r\n",
    "X, y, groups = define_x_y_groups(df)\r\n",
    "training_group = list(set(groups))[:-2]\r\n",
    "#validation_group = list(set(groups))[-8]\r\n",
    "test_group = [list(set(groups))[-2]]\r\n",
    "print(training_group)\r\n",
    "print(test_group)\r\n",
    "training_indices = np.where(groups.isin(training_group))[0]\r\n",
    "#validation_indices = np.where(groups.isin(validation_group))[0]\r\n",
    "test_indices = np.where(groups.isin(test_group))[0]\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = X.iloc[training_indices], X.iloc[\r\n",
    "    test_indices], y.iloc[training_indices], y.iloc[test_indices]\r\n",
    "split_mapping = {\"train\": (X_train, y_train), \"test\": (X_test, y_test)}\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "[64]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-25-58f9736a6512>, line 19)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-58f9736a6512>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    return {k: split_mapping[k] for k in splits}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dataset = data.get_dataset(\r\n",
    "    partial(pd.read_csv, filepath_or_buffer=dataset_path),\r\n",
    "    splits=[\"train\", \"test\"]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**If you need to visualize anything from your training data, do it here**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline\n",
    "\n",
    "Before doing any complex Machine Learning model, let's try to solve the problem by having an initial educated guess. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-24 02-55\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Trying to unpickle estimator OneHotEncoder from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator ColumnTransformer from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator LinearRegression from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Trying to unpickle estimator Pipeline from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Model used:\n",
      "\tPipeline(steps=[('categorical-encoder',\n",
      "                 CategoricalEncoder(force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 119.46145014101693\n",
      "\ttest error: 123.11870799022557\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import shap"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(\"Model coefficients:\\n\")\r\n",
    "X = dataset[\"train\"][0]\r\n",
    "for i in range(X.shape[1]):\r\n",
    "    print(X.columns[i], \"=\", reg.coef_[i].round(4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model coefficients:\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'coef_'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f587e5689959>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'coef_'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shap.plots.partial_dependence(\r\n",
    "    \"RM\", reg.predict, X100, ice=False,\r\n",
    "    model_expected_value=True, feature_expected_value=True\r\n",
    ")\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression Model \n",
    "\n",
    "We want to try easy things first, so know lets see how a linear regression model does."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-24 02-55\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LinearRegression from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "Model used:\n",
      "\tPipeline(steps=[('categorical-encoder',\n",
      "                 CategoricalEncoder(force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 119.46145014101693\n",
      "\ttest error: 123.11870799022557\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort and discretize the errors your model is making and see what the features have in common in those cases. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear regression with Feature Engineering\n",
    "\n",
    "Probably the previous model is not good enough, let's see how is the performance of a model using some produced features.\n",
    "\n",
    "Techniques:\n",
    "1. Feature Cross\n",
    "2. Discretizer\n",
    "3. Add average per neighborhood.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 15-48\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('averager', AveragePricePerNeighborhoodExtractor()),\n",
      "                ('discretizer',\n",
      "                 Discretizer(bins_per_column={'LotArea': 3, 'LotFrontage': 5},\n",
      "                             strategy='quantile')),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_categories={'LotArea': [0.0, 1.0,\n",
      "                                                                       2.0],\n",
      "                                                           'LotFrontage': [0.0,\n",
      "                                                                           1.0,\n",
      "                                                                           2.0,\n",
      "                                                                           3.0,\n",
      "                                                                           4.0]},\n",
      "                                    additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge',\n",
      "                                                                     'AveragePriceInNeihborhood'],\n",
      "                                    force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 11383.190533872741\n",
      "\ttest error: 6505946333741.948\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "Let's assume you are overfitting. Load the results of a linear regression model with regularized loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-24 03-17\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator ColumnTransformer from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Ridge from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 0.24.2 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "Model used:\n",
      "\tPipeline(steps=[('categorical-encoder',\n",
      "                 CategoricalEncoder(force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('ridge-regressor', Ridge(alpha=4.4, solver='saga'))])\n",
      "Error:\n",
      "\ttrain set 118.52788306409211\n",
      "\ttest error: 114.64505744844314\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision trees ofer great complexity, they can fit even a noisy dataset almost perfectly. Let's see how it behaves on the task at hand. \n",
    "\n",
    "**Overfiting case**\n",
    "Let's see the results for a model that has greatly overfit the data, this wouldn't be an ideal model, but at least it could tell that our model is powerful enough for the task at hand"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 15-51\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge'],\n",
      "                                    force_dense_array=True)),\n",
      "                ('decision-tree-regressor', DecisionTreeRegressor())])\n",
      "Error:\n",
      "\ttrain set 0.0\n",
      "\ttest error: 27141.272597526167\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Using best hyper params** Now let's see thow much a simple decision tree can give us"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 15-54\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge'],\n",
      "                                    force_dense_array=True)),\n",
      "                ('decision-tree-regressor',\n",
      "                 DecisionTreeRegressor(max_depth=8, max_features='auto'))])\n",
      "Error:\n",
      "\ttrain set 8884.676197932415\n",
      "\ttest error: 28098.13544441429\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest\n",
    "\n",
    "Now it is time to use a model that can properly help us to regularize the previous one."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 16-08\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge'],\n",
      "                                    force_dense_array=True)),\n",
      "                ('random-forest-regressor',\n",
      "                 RandomForestRegressor(max_depth=18, max_features=20,\n",
      "                                       n_estimators=512))])\n",
      "Error:\n",
      "\ttrain set 5741.839429708461\n",
      "\ttest error: 17727.675141267133\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Catboost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model_path = os.path.join(\"models\", \"2021-07-26 03-38\", \"model.joblib\")\r\n",
    "reg = joblib.load(model_path)\r\n",
    "evaluation = evaluate_model(\r\n",
    "    model=reg,\r\n",
    "    metric=metrics.custom_error,\r\n",
    "    X_train=dataset[\"train\"][0],\r\n",
    "    y_train=dataset[\"train\"][1],\r\n",
    "    X_test=dataset[\"test\"][0],\r\n",
    "    y_test=dataset[\"test\"][1]\r\n",
    ")\r\n",
    "print_report(model=reg, evaluation=evaluation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model used:\n",
      "\t<catboost.core.CatBoostRegressor object at 0x0000027437180D48>\n",
      "Error:\n",
      "\ttrain set 22.671140692503567\n",
      "\ttest error: 81.07222358345156\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "import shap\r\n",
    "explainer = shap.TreeExplainer(reg)\r\n",
    "X_train100=dataset[\"train\"][0][:100]\r\n",
    "shap_values = explainer.shap_values(X_train)\r\n",
    "print(shap_values)\r\n",
    "# # visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\r\n",
    "shap.force_plot(explainer.expected_value,\r\n",
    "                    shap_values[-1], X.iloc[-1], show=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('typer_leo': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "eb15d0076fb97d2cd209a8020ad3c67a674afb401f6957c88b64dabb015d3a1f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}